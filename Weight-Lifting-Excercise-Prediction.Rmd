---
title: "Predicting barbell lift types"
output: html_document
---

## Introduction

In this assignment I will try to predict the manner in which participants performed a weight lifting exercise in the "Weight Lifting Exercise Dataset". The dependent variable is denoted "classe" in the training dataset and can take one of five classifications denoted A, B, C, D and E. The explanatory variables consist of data from accelerometers on the belt, forearm, arm and dumbell. 

I will first examine and preprocess the data. I will explore the scope for reducing the dimensionality of the data. I will then fit a selection of classification models and use these to predict on a validation set. The best performing model will be used to make predictions regarding the way the weights have been lifted for the 20 observations in the original testing set.   

## 1. Examining and pre-processing the data

We read in the data and explore its dimensions.

```{r data, echo = TRUE, warning = FALSE, results = TRUE}
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
dim(training)
```

We see that the dataset contains almost 20,000 observations with 160 variables. Using the str() function (not shown here) we see that many variables contain missing values. We first check how many columns contain NAs and make a plot of how many NAs they contain.

```{r plot_nas, echo = TRUE, warning = FALSE, results = TRUE, fig.height=4}
na <- colSums(is.na(training)); plot(na, main = "Figure 1")
```

Many variables are made up entirely of NAs and can thus be removed.

```{r remove_nas, echo = TRUE, warning = FALSE, results = TRUE}
notna <- colSums(is.na(training)) == 0
notna <- notna[notna == TRUE]
training <- subset(training, select = names(notna))
dim(training)
```

This still, however, leaves us with many character variables that are largely empty. Moreover, the first 4 columns of data are basically indices or time-stamps which will not be useful for our analysis so we remove them. At this stage we also reclassify our target variable "classe" as a factor.

```{r remove_chr, echo = TRUE, warning = FALSE, results = TRUE}
training$classe <- as.factor(training$classe)
training <- training[, !sapply(training, is.character)]
training <- training[,5:57]
dim(training)
```

We are now left with a dataset containing our target variable and 52 explanatory variables with measurements of movement. 

## 2. Dimensionality

With 52 explanatory variables we are still left with a high-dimensional dataset. Examining the correlation between the explanatory variables we see that many are highly correlated

```{r cor, echo = TRUE, warning = FALSE, results = TRUE}
library(kernlab)
M <- abs(cor(training[,-53]))
diag(M) <- 0
X <- sum(M>0.8,arr.in=T)
X
```

There are `r X` cross correlations of explanatory variables higher than 0.8. We therefore explore whether we can reduce dimensionality with principle components analysis. 
```{r pca, echo = TRUE, warning = FALSE, results = TRUE, fig.height=3}
library(caret)
preProc <- preProcess(training[,-53], method=c("center","scale"))
trainingS <- predict(preProc, training)
trainPCA = trainingS[,-53]
pca = prcomp(trainPCA)
pr_var = (pca$sdev)^2 
prop_varex = pr_var/sum(pr_var)
plot(prop_varex, main = "Figure 2", xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", type = "b" )
```

We see, however, that it is difficult to explain the variance in the data with a reasonably small number of principle components. So we will proceed with the full dataset.


## 3. Model fitting, prediction and accuracy 

Our problem is one of classification so we will focus on classification models that can handle the high dimensionality of our dataset. Specifically we will consider: (i) Linear Discriminant Analysis (lda), (ii) K Nearest Neighbours (knn), (iii) Classification Tree (ct); (iv) Boosting (gbm) and (v) Random Forest (rf). 

For cross validation we will use a "validation set" approach in which we reserve a proportion (one-quarter) of our training set for validation. We will train our models on three-quarters of the training set and use them to predict on the remaining quarter (validation set). Given that we have a large sample, this approach has been chosen (as opposed e.g. to k-fold cross validation) to reduce the risk of over-fitting the data.    

```{r validation, echo = TRUE, warning = FALSE, results = FALSE}
library(caret)
set.seed(1234)
inTrain <- createDataPartition(y=training$classe,p=0.75,list=FALSE)
train <- training[inTrain,]
valid <- training[-inTrain,]
```

We now fit, make predictions and store the accuracy statistic for each model. A full summary of the predictive performance of each model is reported in the Annex. 

(i) Linear Discriminant Analysis
```{r fitting_lda, echo = TRUE, warning = FALSE, results = FALSE}
# Linear Discriminant Analysis
library(klaR); mod_lda = train(classe ~ .,data=train,method="lda")
predict(mod_lda,valid)
lda_acc <- confusionMatrix(valid$classe,predict(mod_lda,valid))$overall[1]
```

(ii) K Nearest Neighbours
```{r fitting_knn, echo = TRUE, warning = FALSE, results = FALSE}
# K Nearest Neighbours
mod_knn = train(classe ~ .,data=train,method="knn")
predict(mod_knn,valid); 
knn_acc <- confusionMatrix(valid$classe,predict(mod_knn,valid))$overall[1]
```

(iii) Classification Tree
```{r fitting_ct, echo = TRUE, warning = FALSE, results = FALSE}
# Classification Tree
mod_ct <- train(classe ~.,method = "rpart",data=train)
predict(mod_ct,valid)
ct_acc <- confusionMatrix(valid$classe,predict(mod_ct,valid))$overall[1]
```

(iv) Boosting
```{r fitting_gbm, echo = TRUE, warning = FALSE, results = FALSE}
# Boosting
library(gbm); set.seed(1234) 
mod_gbm <- train(classe ~ ., data=train,method="gbm", verbose=FALSE)
predict(mod_gbm,valid)
gbm_acc <- confusionMatrix(valid$classe,predict(mod_gbm,valid))$overall[1]        
```

(v) Random Forest
```{r fitting_rf, echo = TRUE, warning = FALSE, results = FALSE}
# Random Forest
library(randomForest); set.seed(1234)
mod_rf <- train(classe ~ ., data=train,method="rf")
predict(mod_rf,valid)
rf_acc <- confusionMatrix(valid$classe,predict(mod_rf,valid))$overall[1] 
```


The accuracy of each model is as follows:

```{r results, echo = TRUE, warning = FALSE, results = TRUE}
results <- as.data.frame(list(lda = lda_acc, knn = knn_acc, ct = ct_acc,
                              gbm = gbm_acc, rf = rf_acc))
results
rf_error <- 1 - rf_acc
```

We see that all models do seem to help predict the manner in which the excercise was done, but that in relative terms the performance of the classification tree and of the linear discriminant analysis is poor. The k-nearest neighbours method and boosting perform well, predicting correctly more than 90% of the observations in the validation set. However, clearly the best performing model is the random forest with an accuracy of `r rf_acc`, which implies an out of sample error of just `r rf_error`.  

The full prediction results for this model are shown below. Those for the other models are shown in the Annex.

### Prediction performance of the best model (random forest)
```{r best, echo = TRUE, warning = FALSE, results = TRUE}
confusionMatrix(valid$classe,predict(mod_rf,valid))
```

## 4. Final prediction on the testing data

We now use our final model to make our prediction on the original testing data.

```{r predict, echo = TRUE, warning = FALSE, results = TRUE}
pred <- predict(mod_rf,testing)
pred
```


# ANNEX

## Our dataset after having removed irrelevant variables
```{r str, echo = TRUE, warning = FALSE, results = TRUE}
str(training)
```

## Prediction performance of different models

### Linear discriminant analysis 
```{r lda, echo = FALSE, warning = FALSE, results = TRUE}
confusionMatrix(valid$classe,predict(mod_lda,valid))
```

### K nearest neighbours 
```{r knn, echo = FALSE, warning = FALSE, results = TRUE}
confusionMatrix(valid$classe,predict(mod_knn,valid))
```

### Classification tree 
```{r ct, echo = FALSE, warning = FALSE, results = TRUE}
confusionMatrix(valid$classe,predict(mod_ct,valid))
```

### Boosting 
```{r gbm, echo = FALSE, warning = FALSE, results = TRUE}
confusionMatrix(valid$classe,predict(mod_gbm,valid))
```
